{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905ff620",
   "metadata": {},
   "source": [
    "# Flatiron Capstone Project – Notebook #1: Data Prep\n",
    "\n",
    "Student name: **Angelo Turri**\n",
    "\n",
    "Student pace: **self paced**\n",
    "\n",
    "Project finish date: **1/19/24**\n",
    "\n",
    "Instructor name: **Mark Barbour**\n",
    "\n",
    "# Instructions\n",
    "\n",
    "Due to the size of this project, there are four notebooks instead of one. The proper order to execute these notebooks is as follows:\n",
    "\n",
    "- Gathering Data **<---- You are here**\n",
    "- Preprocessing\n",
    "- Feature Engineering\n",
    "- Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99233a6b",
   "metadata": {},
   "source": [
    "### Stakeholder\n",
    "Your stakeholder is a social media communications team working for a political candidate, Donald Trump. They have requested that you analyze a body of social media posts from their voter base and extract meaningful insights on their base's attitudes.\n",
    "\n",
    "### Data: Origin\n",
    "Data is taken from the former reddit titled ***r/the_donald***. This reddit has been archived along with 20,000 others on [the-eye.eu](https://the-eye.eu/redarcs/). If you want to download it yourself, you just need to type \"the_donald\" in the search bar on this website and download the \"Comments\" link provided there. However, this notebook should download the file for you.\n",
    "\n",
    "Fair warning - if you are about to explore on this website, be cautious. I looked at some of the archived reddits and will never be the same again.\n",
    "\n",
    "### Data: Statistics\n",
    "The compressed file is sizeable at 3.8GB, but this is in .zst format. Once converted to a .txt file, it takes up a whopping 37.48 GB of space, containing data on approximately 48 million posts. Due to the sheer amount of data and the limitations of my machine, I was unwilling to analyze all 48 million posts. I wanted to take 2 million posts, so I kept every 23rd post from this file.\n",
    "\n",
    "Each post is recorded as a dictionary. Only some of the keys were relevant to our analysis:\n",
    "- Raw text\n",
    "- Post score (upvotes - downvotes)\n",
    "- Author\n",
    "- Date posted\n",
    "\n",
    "After extraction, our initial dataframe had 2.1 million total entries ranging from August of 2015 to April of 2020, for a total of 1710 days – approximately 4.5 years. There are 178,308 unique authors.\n",
    "\n",
    "\n",
    "### Basic spam removal\n",
    "Thorough spam removal occurs in the **preprocessing notebook**, but we conduct some very basic spam removal in this notebook. Any removed posts (whether deleted by moderators, or authors), and empty posts are removed. All in all, 198,237 of our original posts were removed due to these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b1d4c",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bcd434f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import urllib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9c37c",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1879fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name='the_donald_comments'\n",
    "path = 'data/the_donald_comments.zst'\n",
    "\n",
    "exists = os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4dd68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed .zst file for the_donald_comments already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "name='the_donald_comments'\n",
    "path = '../data/the_donald_comments.zst'\n",
    "\n",
    "# Downloading can be a lengthy process. If you have already downloaded the file,\n",
    "# this cell will not re-download.\n",
    "exists = os.path.exists(path)\n",
    "\n",
    "if exists:\n",
    "    print(f\"Compressed .zst file for {name} already exists. Skipping download.\")\n",
    "else:\n",
    "    url = 'https://the-eye.eu/redarcs/files/The_Donald_comments.zst'\n",
    "    urllib.request.urlretrieve(url, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9804c52",
   "metadata": {},
   "source": [
    "### Converting .zst file to a .txt file\n",
    "\n",
    "The raw data is in a compressed .zst format, and it has to be un-compressed. I stream-read the file to avoid crashing my kernel. The file is huge – after it gets uncompressed, it takes up almost 38GB of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be29bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f\"../data/{name}.zst\"\n",
    "output_path = f\"../data/{name}.txt\"\n",
    "exists = os.path.exists(output_path)\n",
    "\n",
    "\n",
    "# Decompression can take time and is avoided if the text file already exists.\n",
    "if exists:\n",
    "    print(f\"Output file for {name}_comments already exists. Decompression skipped.\")\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    print(f\"Output file for {name}_comments does not exist. Commencing decompression.\")\n",
    "\n",
    "    # Get the size of the input file\n",
    "    input_path_size = os.path.getsize(input_path)\n",
    "\n",
    "    # Open the input file in binary read mode\n",
    "    with open(input_path, 'rb') as compressed_path:\n",
    "        # Create a ZstdDecompressor object\n",
    "        decompressor = zstd.ZstdDecompressor()\n",
    "\n",
    "        # Create a decompression stream reader\n",
    "        with decompressor.stream_reader(compressed_path) as reader:\n",
    "            # Open the output file in binary write mode\n",
    "            with open(output_path, 'wb') as decompressed_path:\n",
    "                # Initialize tqdm with total size\n",
    "                with tqdm(total=input_path_size, unit='B', unit_scale=True, desc='Decompressing', leave=True) as pbar:\n",
    "                    # Read and decompress data in chunks\n",
    "                    while True:\n",
    "                        # Read a chunk of data from the decompression stream\n",
    "                        chunk = reader.read(65536)  # Read 64KB at a time\n",
    "\n",
    "                        # Check if there's no more data to read\n",
    "                        if not chunk:\n",
    "                            break\n",
    "\n",
    "                        # Write the decompressed chunk to the output file\n",
    "                        decompressed_path.write(chunk)\n",
    "\n",
    "                        # Update progress bar\n",
    "                        pbar.update(len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8cbb7",
   "metadata": {},
   "source": [
    "### Extracting important information from the .txt file\n",
    "\n",
    "Now that we've uncompressed the file, it's time to start extracting information. I am not analyzing all 38GB of this file. Instead, I am taking every 23rd post for analysis, which comes up to about 2,000,000 posts. This is more than enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for older posts\n",
    "old_lines = []\n",
    "ups = []\n",
    "downs = []\n",
    "old_scores = []\n",
    "\n",
    "# Lists for newer posts\n",
    "lines = []\n",
    "scores = []\n",
    "authors = []\n",
    "utcs = []\n",
    "posts = []\n",
    "\n",
    "with open(f\"../data/{name}.txt\", 'r') as txt_file:\n",
    "    lines_read = 0\n",
    "    \n",
    "    for line in tqdm(txt_file, desc = f\"Extracting text from\"):\n",
    "        data = json.loads(line.strip())\n",
    "        lines_read +=1\n",
    "        \n",
    "        if lines_read % 23 == 0:\n",
    "            try:\n",
    "                downs.append(data['downs'])\n",
    "                old_scores.append(data['score'])\n",
    "                ups.append(data['ups'])\n",
    "                old_lines.append(data)\n",
    "            except:\n",
    "                lines.append(data)\n",
    "                scores.append(data['score'])\n",
    "                authors.append(data['author'])\n",
    "                utcs.append(data['created_utc'])\n",
    "                posts.append(data['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3650c9d4",
   "metadata": {},
   "source": [
    "### Proving that \"score\" is equal to upvotes minus downvotes\n",
    "\n",
    "Older Reddit posts have upvotes and downvotes recorded separately as well as the total post score. Newer Reddit posts only have the total post score recorded.\n",
    "\n",
    "For every posts with upvotes and downvotes recorded, I subtracted the number of downvotes from the number of upvotes and compared it to the total score. For all 45,529 posts, they were the same. This proves that the \"score\" attribute of the Reddit posts is equal to the number of upvotes minus the number of downvotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of an older post\n",
    "old_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eacf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a newer post\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b38d9be",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4218242034.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    joblib.dump(lines[0], f)b\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with open(f'data/sample_post.pkl', 'wb') as f:\n",
    "    joblib.dump(lines[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7776520c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   upvotes  downvotes  score\n",
       "0        1          0      1\n",
       "1        0          0      0\n",
       "2        2          0      2\n",
       "3        1          0      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_posts_df = pd.DataFrame({'upvotes': ups, 'downvotes': downs, 'score': old_scores})\n",
    "old_posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003d025e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For every entry, the score is equal to the upvotes minus the downvotes.\n",
    "(old_posts_df['upvotes'] - old_posts_df['downvotes'] == old_posts_df['score']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94bc908",
   "metadata": {},
   "source": [
    "### Creating initial dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73402e82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'date': pd.to_datetime(utcs, unit='s', utc=True), \n",
    "                   'author': authors, \n",
    "                   'post': posts, \n",
    "                   'score': scores})\n",
    "\n",
    "df.index = df.index.rename('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a7c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>post</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-08-05 22:19:44+00:00</td>\n",
       "      <td>NYPD-32</td>\n",
       "      <td>A lot of latinos are annoyed with illegal immi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-08-09 23:51:28+00:00</td>\n",
       "      <td>shitheadsean2</td>\n",
       "      <td>If Donald Trump liquidated everything, and the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-08-13 19:16:16+00:00</td>\n",
       "      <td>NYPD-32</td>\n",
       "      <td>An*</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-08-14 16:38:24+00:00</td>\n",
       "      <td>the_achiever</td>\n",
       "      <td>I really support Trump, but he has to work on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-08-17 14:53:52+00:00</td>\n",
       "      <td>Degenerate_Nation</td>\n",
       "      <td>Two-part Trump-centric podcasts:\\n\\nhttp://www...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093201</th>\n",
       "      <td>2020-04-09 21:21:21+00:00</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093202</th>\n",
       "      <td>2020-04-10 04:26:11+00:00</td>\n",
       "      <td>Fordheartskav</td>\n",
       "      <td>yes and yes. So many spez suckers lying about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093203</th>\n",
       "      <td>2020-04-10 15:57:11+00:00</td>\n",
       "      <td>RhettOracle</td>\n",
       "      <td>Now it's brutality?   You are so biased it's h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093204</th>\n",
       "      <td>2020-04-11 03:00:14+00:00</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093205</th>\n",
       "      <td>2020-04-11 18:49:38+00:00</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2093206 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date             author  \\\n",
       "id                                                     \n",
       "0       2015-08-05 22:19:44+00:00            NYPD-32   \n",
       "1       2015-08-09 23:51:28+00:00      shitheadsean2   \n",
       "2       2015-08-13 19:16:16+00:00            NYPD-32   \n",
       "3       2015-08-14 16:38:24+00:00       the_achiever   \n",
       "4       2015-08-17 14:53:52+00:00  Degenerate_Nation   \n",
       "...                           ...                ...   \n",
       "2093201 2020-04-09 21:21:21+00:00          [deleted]   \n",
       "2093202 2020-04-10 04:26:11+00:00      Fordheartskav   \n",
       "2093203 2020-04-10 15:57:11+00:00        RhettOracle   \n",
       "2093204 2020-04-11 03:00:14+00:00          [deleted]   \n",
       "2093205 2020-04-11 18:49:38+00:00          [deleted]   \n",
       "\n",
       "                                                      post  score  \n",
       "id                                                                 \n",
       "0        A lot of latinos are annoyed with illegal immi...      2  \n",
       "1        If Donald Trump liquidated everything, and the...      5  \n",
       "2                                                      An*      7  \n",
       "3        I really support Trump, but he has to work on ...      1  \n",
       "4        Two-part Trump-centric podcasts:\\n\\nhttp://www...      1  \n",
       "...                                                    ...    ...  \n",
       "2093201                                          [deleted]      1  \n",
       "2093202  yes and yes. So many spez suckers lying about ...      1  \n",
       "2093203  Now it's brutality?   You are so biased it's h...      1  \n",
       "2093204                                          [removed]     17  \n",
       "2093205                                          [deleted]      1  \n",
       "\n",
       "[2093206 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A preview at the data and the number of rows – 2.1 million\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0bf8427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2093206 entries, 0 to 2093205\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Dtype              \n",
      "---  ------  -----              \n",
      " 0   date    datetime64[ns, UTC]\n",
      " 1   author  object             \n",
      " 2   post    object             \n",
      " 3   score   int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(2)\n",
      "memory usage: 63.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# The dtypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff426ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest post is 2015-08-05 22:19:44+00:00.\n",
      "The latest post is 2020-04-11 18:49:38+00:00.\n",
      "The data spans 1710 days 20:29:54.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The earliest post is {df.date.min()}.\")\n",
    "print(f\"The latest post is {df.date.max()}.\")\n",
    "print(f\"The data spans {df.date.max() - df.date.min()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b064188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains 178308 unique authors.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data contains {df.author.nunique()} unique authors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51261de",
   "metadata": {},
   "source": [
    "# Storing data externally\n",
    "\n",
    "Now we have collected all the essential data for analysis. We are going to store the data externally and pick up from there in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b76c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(path=f'../data/{name}.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
